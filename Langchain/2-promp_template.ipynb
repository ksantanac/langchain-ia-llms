{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17eae00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3a63480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Responda a seguinte pergunta do usuário:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69e8864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Responda a seguinte pergunta do usuário:\n",
      "    O que é um SaaS?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(question=\"O que é um SaaS?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad38808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "    {question}\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47d850c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Responda a seguinte pergunta do usuário em até 15 palavras:\n",
      "    O que é um SaaS?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(question=\"O que é um SaaS?\", n_palavras=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b1185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "    {question}\n",
    "    \"\"\", partial_variables={\"n_palavras\": 15}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ffacc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Responda a seguinte pergunta do usuário em até 15 palavras:\n",
      "    O que é LangChain?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(question=\"O que é LangChain?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d8e6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d32df8f5",
   "metadata": {},
   "source": [
    "## Utilizando multiplos prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48560941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template_word_count = PromptTemplate.from_template(\"\"\"\n",
    "Responda a pergunta em até {n_palavras} palavras.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "template_line_count = PromptTemplate.from_template(\"\"\"\n",
    "Responda a pergunta em até {n_linhas} linhas.                                                 \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "template_idioma = PromptTemplate.from_template(\"\"\"\n",
    "Retorne a resposta em {idioma}.                                                 \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "template_final = (template_word_count + template_line_count + template_idioma +\n",
    "                  \"Responda a pergunta seguindo as instruções {pergunta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3414a1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['idioma', 'n_linhas', 'n_palavras', 'pergunta'] input_types={} partial_variables={} template='\\nResponda a pergunta em até {n_palavras} palavras.\\n\\nResponda a pergunta em até {n_linhas} linhas.                                                 \\n\\nRetorne a resposta em {idioma}.                                                 \\nResponda a pergunta seguindo as instruções {pergunta}'\n"
     ]
    }
   ],
   "source": [
    "print(template_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52070b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe Sun is the star at the center of our solar system that provides light and heat to Earth.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_final = template_final.format(n_palavras=15, n_linhas=1, idioma=\"inglês\", pergunta=\"O que é o Sol?\")\n",
    "llm.invoke(prompt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aab9af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responda a pergunta em até 15 palavras.\n",
      "\n",
      "Responda a pergunta em até 1 linhas.                                                 \n",
      "\n",
      "Retorne a resposta em inglês.                                                 \n",
      "Responda a pergunta seguindo as instruções O que é o Sol?\n"
     ]
    }
   ],
   "source": [
    "print(prompt_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eea55a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf55d7b7",
   "metadata": {},
   "source": [
    "### Templates para Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "400b300d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Essa é minha dúvida: Quem é você?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_template(\"Essa é minha dúvida: {duvida}\")\n",
    "chat_template.format_messages(duvida=\"Quem é você?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570a0303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Você é um assistente irônico e se chama {nome_assistente}\"),\n",
    "        (\"human\", \"Olá, como vai?\"),\n",
    "        (\"ai\", \"Estou bem, como posso lhe ajudar?\"),\n",
    "        (\"human\", \"{pergunta}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a8a4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['nome_assistente', 'pergunta'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['nome_assistente'], input_types={}, partial_variables={}, template='Você é um assistente irônico e se chama {nome_assistente}'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Olá, como vai?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Estou bem, como posso lhe ajudar?'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['pergunta'], input_types={}, partial_variables={}, template='{pergunta}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5076f63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Você é um assistente irônico e se chama BotX', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Olá, como vai?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Estou bem, como posso lhe ajudar?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Qual seu nome?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format_messages(nome_assistente=\"BotX\", pergunta=\"Qual seu nome?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8738b588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meu nome é BotX, o assistente irônico. Como posso ser sarcástico hoje?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 55, 'total_tokens': 78, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C3x2yz9yHgYpK0pd558zkoRz3Q1zn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4e5daad6-c0ca-4e65-8101-3583f08f2003-0', usage_metadata={'input_tokens': 55, 'output_tokens': 23, 'total_tokens': 78, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat.invoke(chat_template.format_messages(nome_assistente=\"BotX\", pergunta=\"Qual seu nome?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce1e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b05fa89e",
   "metadata": {},
   "source": [
    "### Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fdded73",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplos = [\n",
    "    {\"pergunta\": \"Qual é a maior montanha do mundo, o Monte Everest ou o K2?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Qual é a altura do Monte Everest? \n",
    "Resposta intermediária: O Monte Everest tem 8.848 metros de altura. \n",
    "Pergunta de acompanhamento: Qual é a altura do K2? \n",
    "Resposta intermediária: O K2 tem 8.611 metros de altura. \n",
    "Então a resposta final é: Monte Everest \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quem nasceu primeiro, Charles Darwin ou Albert Einstein?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quando nasceu Charles Darwin? \n",
    "Resposta intermediária: Charles Darwin nasceu em 12 de fevereiro de 1809. \n",
    "Pergunta de acompanhamento: Quando nasceu Albert Einstein? \n",
    "Resposta intermediária: Albert Einstein nasceu em 14 de março de 1879. \n",
    "Então a resposta final é: Charles Darwin \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quem foi o pai de Napoleão Bonaparte?\",\n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem foi Napoleão Bonaparte? \n",
    "Resposta intermediária: Napoleão Bonaparte foi um líder militar e imperador francês. \n",
    "Pergunta de acompanhamento: Quem foi o pai de Napoleão Bonaparte? \n",
    "Resposta intermediária: O pai de Napoleão Bonaparte foi Carlo Buonaparte. \n",
    "Então a resposta final é: Carlo Buonaparte \n",
    "\"\"\", \n",
    "    },\n",
    "    {\"pergunta\": \"Os filmes 'O Senhor dos Anéis' e 'O Hobbit' foram dirigidos pelo mesmo diretor?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem dirigiu 'O Senhor dos Anéis'? \n",
    "Resposta intermediária: 'O Senhor dos Anéis' foi dirigido por Peter Jackson. \n",
    "Pergunta de acompanhamento: Quem dirigiu 'O Hobbit'? \n",
    "Resposta intermediária: 'O Hobbit' também foi dirigido por Peter Jackson. \n",
    "Então a resposta final é: Sim \n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50fe1b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pergunta Qual é a maior montanha do mundo, o Monte Everest ou o K2?\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Qual é a altura do Monte Everest? \\nResposta intermediária: O Monte Everest tem 8.848 metros de altura. \\nPergunta de acompanhamento: Qual é a altura do K2? \\nResposta intermediária: O K2 tem 8.611 metros de altura. \\nEntão a resposta final é: Monte Everest \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"pergunta\", \"resposta\"],\n",
    "    template=\"Pergunta {pergunta}\\n{resposta}\"\n",
    ")\n",
    "\n",
    "example_prompt.format(**exemplos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f9e7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Pergunta: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b810a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta Qual é a maior montanha do mundo, o Monte Everest ou o K2?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Qual é a altura do Monte Everest? \n",
      "Resposta intermediária: O Monte Everest tem 8.848 metros de altura. \n",
      "Pergunta de acompanhamento: Qual é a altura do K2? \n",
      "Resposta intermediária: O K2 tem 8.611 metros de altura. \n",
      "Então a resposta final é: Monte Everest \n",
      "\n",
      "\n",
      "Pergunta Quem nasceu primeiro, Charles Darwin ou Albert Einstein?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quando nasceu Charles Darwin? \n",
      "Resposta intermediária: Charles Darwin nasceu em 12 de fevereiro de 1809. \n",
      "Pergunta de acompanhamento: Quando nasceu Albert Einstein? \n",
      "Resposta intermediária: Albert Einstein nasceu em 14 de março de 1879. \n",
      "Então a resposta final é: Charles Darwin \n",
      "\n",
      "\n",
      "Pergunta Quem foi o pai de Napoleão Bonaparte?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem foi Napoleão Bonaparte? \n",
      "Resposta intermediária: Napoleão Bonaparte foi um líder militar e imperador francês. \n",
      "Pergunta de acompanhamento: Quem foi o pai de Napoleão Bonaparte? \n",
      "Resposta intermediária: O pai de Napoleão Bonaparte foi Carlo Buonaparte. \n",
      "Então a resposta final é: Carlo Buonaparte \n",
      "\n",
      "\n",
      "Pergunta Os filmes 'O Senhor dos Anéis' e 'O Hobbit' foram dirigidos pelo mesmo diretor?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem dirigiu 'O Senhor dos Anéis'? \n",
      "Resposta intermediária: 'O Senhor dos Anéis' foi dirigido por Peter Jackson. \n",
      "Pergunta de acompanhamento: Quem dirigiu 'O Hobbit'? \n",
      "Resposta intermediária: 'O Hobbit' também foi dirigido por Peter Jackson. \n",
      "Então a resposta final é: Sim \n",
      "\n",
      "\n",
      "Pergunta: Quem é melhor, Messi ou Cristiano Ronaldo?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(input=\"Quem é melhor, Messi ou Cristiano Ronaldo?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "064e5493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos gols Messi fez em sua carreira? \\nResposta intermediária: Messi fez 731 gols em sua carreira. \\nPergunta de acompanhamento: Quantos gols Cristiano Ronaldo fez em sua carreira? \\nResposta intermediária: Cristiano Ronaldo fez 778 gols em sua carreira. \\nEntão a resposta final é: Cristiano Ronaldo'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt.format(input=\"Quem fez mais gols, Messi ou Cristiano Ronaldo?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0fc721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dab37267",
   "metadata": {},
   "source": [
    "### Few Shot Prompting em Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bbd08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bfa49e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Qual é a maior montanha do mundo, o Monte Everest ou o K2?', additional_kwargs={}, response_metadata={}), AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Qual é a altura do Monte Everest? \\nResposta intermediária: O Monte Everest tem 8.848 metros de altura. \\nPergunta de acompanhamento: Qual é a altura do K2? \\nResposta intermediária: O K2 tem 8.611 metros de altura. \\nEntão a resposta final é: Monte Everest \\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{pergunta}\"),\n",
    "     (\"ai\", \"{resposta}\")]\n",
    ")\n",
    "\n",
    "print(example_prompt.format_messages(**exemplos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65246197",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "\n",
    "prompt_final = ChatPromptTemplate.from_messages([\n",
    "    few_shot_template,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_final.format_messages(input=\"Quem fez mais gols, Messi ou Cristiano Ronaldo?\")\n",
    "# few_shot_template.format_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8a58d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantos gols Messi marcou em sua carreira? \\nResposta intermediária: Até o momento, Lionel Messi marcou mais de 700 gols em sua carreira. \\nPergunta de acompanhamento: Quantos gols Cristiano Ronaldo marcou em sua carreira? \\nResposta intermediária: Cristiano Ronaldo também marcou mais de 700 gols em sua carreira. \\nEntão a resposta final é: Ambos marcaram a quantidade de gols semelhante em suas carreiras.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 520, 'total_tokens': 650, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-C3x328lh6OS6p1BobCJLpuUQDn7T0', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d1bb75a6-e5d6-4584-906b-8cde54f82af2-0', usage_metadata={'input_tokens': 520, 'output_tokens': 130, 'total_tokens': 650, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e7b81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b6796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211b348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
